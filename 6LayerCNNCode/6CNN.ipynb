{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "#from tensorflow.keras.models import load_model\n",
    "from keras.models import load_model\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.00025\n",
    "epsilon = 0.1  # Exploration rate\n",
    "episodes = 1  # Number of training episodes\n",
    "optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
    "loss_function = 'mse'  # Mean Squared Error loss for Q-value difference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn(input_shape, num_actions):\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=input_shape),\n",
    "        layers.Conv2D(64, (8, 8), strides=(4, 4), activation='relu'),\n",
    "        layers.Conv2D(128, (4, 4), strides=(2, 2), activation='relu'),\n",
    "        layers.Conv2D(256, (3, 3), strides=(2, 2), activation='relu'),  # First 256 layer\n",
    "        layers.Conv2D(256, (3, 3), activation='relu'),  # Second 256 layer\n",
    "        layers.Conv2D(256, (3, 3), activation='relu'),  # Third 256 layer\n",
    "        layers.Conv2D(256, (3, 3), activation='relu'),  # Fourth 256 layer\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.Dense(num_actions, activation='linear')\n",
    "    ])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('ALE/Frogger-v5', render_mode='rgb_array')\n",
    "#env = gym.make('ALE/Frogger-v5', )\n",
    "input_shape = env.observation_space.shape  # This should match the frame size\n",
    "num_actions = env.action_space.n  # Number of possible actions\n",
    "\n",
    "model = create_cnn(input_shape, num_actions)\n",
    "model.compile(optimizer=optimizers.Adam(learning_rate=0.00025), loss='mse')  # Mean Squared Error loss for Q-value difference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, episodes, save_interval=10):\n",
    "    # Initialize list to keep track of total rewards for each episode\n",
    "    episode_rewards = []\n",
    "\n",
    "    # Create a directory to save model weights\n",
    "    save_dir = \"model_weights\"\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    # Set up tqdm progress bar\n",
    "    with tqdm(total=episodes, desc=\"Episode\", unit='episode') as pbar:\n",
    "        for e in range(episodes):\n",
    "            state = env.reset()[0]\n",
    "            state = np.array(state)\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "\n",
    "            while not done:\n",
    "                # Randomly choose an action or the best predicted action\n",
    "                if np.random.rand() <= epsilon:  # Use the global epsilon value\n",
    "                    action = env.action_space.sample()\n",
    "                else:\n",
    "                    q_values = model.predict(np.expand_dims(state, axis=0), verbose=0)\n",
    "                    action = np.argmax(q_values[0])\n",
    "\n",
    "                next_state, reward, terminated, truncated, info = env.step(action)\n",
    "                next_state = np.array(next_state)\n",
    "                total_reward += reward\n",
    "                \n",
    "                if terminated or truncated:\n",
    "                    done = True\n",
    "\n",
    "            # Update progress bar\n",
    "            pbar.update(1)\n",
    "            pbar.set_description(f\"Episode: {e+1}, Reward: {total_reward}\")\n",
    "\n",
    "            # Append the total reward to the rewards list\n",
    "            episode_rewards.append(total_reward)\n",
    "\n",
    "            # Save the model every 'save_interval' episodes\n",
    "            if (e + 1) % save_interval == 0:\n",
    "                model_path = os.path.join(save_dir, f'model_episode_{e + 1}V2.h5')\n",
    "                model.save(model_path)\n",
    "                print(f\"Saved model at episode {e + 1} to {model_path}\")\n",
    "\n",
    "    # Print overall training results\n",
    "    print(f\"Average Reward: {np.mean(episode_rewards)}\")\n",
    "    print(f\"Best Reward: {max(episode_rewards)}\")\n",
    "# Example usage\n",
    "train_model(model, 500)  # Adjust as needed for your setup\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_models(weights_folder, input_shape, num_actions, episodes_per_model=50):\n",
    "    weight_files = [f for f in os.listdir(weights_folder) if f.endswith('.h5')]\n",
    "    \n",
    "    for weight_file in weight_files:\n",
    "        model = create_cnn(input_shape, num_actions)  # Re-create the model architecture\n",
    "        model_path = os.path.join(weights_folder, weight_file)\n",
    "        model.load_weights(model_path)  # Load the weights\n",
    "        print(f\"Testing model with weights: {weight_file}\")\n",
    "\n",
    "        total_rewards = []\n",
    "        total_times = []\n",
    "\n",
    "        for _ in range(episodes_per_model):\n",
    "            start_time = time.time()\n",
    "            observation, info = env.reset()  # Unpack the tuple to get the observation and info\n",
    "            state = np.array(observation)\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "\n",
    "            while not done:\n",
    "                q_values = model.predict(np.expand_dims(state, axis=0), verbose=0)\n",
    "                action = np.argmax(q_values[0])\n",
    "                next_observation, reward, terminated, truncated, info = env.step(action)\n",
    "                next_state = np.array(next_observation)\n",
    "                total_reward += reward\n",
    "                if terminated or truncated:\n",
    "                    done = True\n",
    "                state = next_state\n",
    "\n",
    "            episode_time = time.time() - start_time\n",
    "            total_rewards.append(total_reward)\n",
    "            total_times.append(episode_time)\n",
    "\n",
    "        average_reward = np.mean(total_rewards)\n",
    "        average_time = np.mean(total_times)\n",
    "        print(f\"Average Reward: {average_reward:.2f}, Average Time: {average_time:.2f} seconds per episode\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gym' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mgym\u001b[49m\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mALE/Frogger-v5\u001b[39m\u001b[38;5;124m'\u001b[39m, render_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m input_shape \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mobservation_space\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m      3\u001b[0m num_actions \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mn\n",
      "\u001b[1;31mNameError\u001b[0m: name 'gym' is not defined"
     ]
    }
   ],
   "source": [
    "env = gym.make('ALE/Frogger-v5', render_mode = 'human')\n",
    "input_shape = env.observation_space.shape\n",
    "num_actions = env.action_space.n\n",
    "test_models(\"model_weights/V2\", input_shape, num_actions, 1)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
